---
title: "Pset V"
author: "Summer Negahdar & Jenny Zhong"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
      text: |
        \usepackage[margin=0.7in]{geometry}
        \usepackage{fvextra}
        \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
execute:
  code-fold: True

---
Partner 1: Summer Negahdar(samarneg)
Partner 2: 
This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: **__** **__**
“I have uploaded the names of anyone else other than my partner and I worked with on
the problem set here”
Late coins used this pset: **__** Late coins left after submission: **__**

### A NOTE TO INSTRUCTORS

working on writing the function to scrape and crawl, we found the questions very ocnfusing in terms of task allocation to each partner and therefore instead of writing one short loop for first part and then writing a wholistic loop to scrape all pages we only wrote one gigantic function to scrape everything!

## Develop Initial scraper and crawler

### 1.

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
import time

def fetch_page_data(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    data = []
    actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')
    for action in actions:
        title_tag = action.find('h2', class_='usa-card__heading')
        title = title_tag.get_text(strip=True) if title_tag else 'No Title Provided'
        
        link = title_tag.find('a')['href'] if title_tag and title_tag.find('a') else 'No Link Provided'
        link = f"https://oig.hhs.gov{link}" if link.startswith('/') else link

        # Correctly locate the date using the new structure
        date_div = action.find('div', class_='font-body-sm margin-top-1')
        date = date_div.find('span', class_='text-base-dark padding-right-105').get_text(strip=True) if date_div else 'No Date Provided'
        
        # Correctly locate the category using the new structure
        category_ul = action.find('ul', class_='display-inline add-list-reset')
        category = category_ul.find('li').get_text(strip=True) if category_ul and category_ul.find('li') else 'No Category Provided'

        data.append({'Title': title, 'Date': date, 'Category': category, 'Link': link})
    return data

def scrape_all_pages(base_url, num_pages):
    urls = [f"{base_url}?page={i}" for i in range(1, num_pages + 1)]
    all_data = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(fetch_page_data, urls))
    for result in results:
        all_data.extend(result)
    return pd.DataFrame(all_data)

# Base URL of the site to scrape
base_url = 'https://oig.hhs.gov/fraud/enforcement/'

num_pages = 482  

# Scrape the data
final_df = scrape_all_pages(base_url, num_pages)

print(final_df.head(-5))
print( "number of items found is", len(final_df))
```

### 2.

```{python}
from concurrent.futures import ThreadPoolExecutor, as_completed

def fetch_agency(link):
    try:
        response = requests.get(link)  # Request the page using the link
        soup = BeautifulSoup(response.text, 'html.parser')  # Parse the content of the page

        # Find all <ul> elements with the specified class containing the agency details
        uls = soup.find_all("ul", class_="usa-list usa-list--unstyled margin-y-2")
        agency_name = 'N/A'  # Default if no agency info is found

        # Iterate over each <ul> element
        for ul in uls:
            spans = ul.find_all("span", class_="padding-right-2 text-base")
            if len(spans) > 1:
                agency = spans[1]  # Select the second <span>, which contains "Agency:"
                agency_name = agency.next_sibling.strip() if agency.next_sibling else 'N/A'
                break  # Stop after finding the first matching <ul> and <span> structure

        return agency_name

    except requests.exceptions.RequestException as e:
        print(f"Error fetching {link}: {e}")
        return 'N/A'

def clean_agency_data(agency):
    # Check if the agency data contains a semicolon which might indicate the inclusion of a date
    if ';' in agency:
        parts = agency.split(';')
        if len(parts) > 1:
            return parts[1].strip()
    return agency.strip()

def get_agencies(df):
    agencies = ['N/A'] * len(df)  # Initialize a list of 'N/A' for all agencies
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_index = {executor.submit(fetch_agency, row['Link']): index for index, row in df.iterrows() if row['Link']}
        for future in as_completed(future_to_index):
            index = future_to_index[future]
            raw_agency = future.result()
            cleaned_agency = clean_agency_data(raw_agency)  # Clean the agency data
            agencies[index] = cleaned_agency
    return agencies

def scrape_all_pages(base_url, num_pages):
    urls = [f"{base_url}?page={i}" for i in range(1, num_pages + 1)]
    all_data = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(fetch_agency, urls))
    for result in results:
        all_data.extend(result)
    return pd.DataFrame(all_data)

# Base URL of the site to scrape
base_url = 'https://oig.hhs.gov/fraud/enforcement/'

# Determine the number of pages or set a fixed number if known
num_pages = 482 

# Scrape the data
final_df = scrape_all_pages(base_url, num_pages)

# Add cleaned agency data to DataFrame
final_df['Agency'] = get_agencies(final_df)

# Print the first few rows of the DataFrame to check
print(final_df.head())

# Save the DataFrame to a CSV file
final_df.to_csv("enforcement_actions_all_pages.csv", index=False)
print("Data scraped and saved to enforcement_actions_all_pages.csv")

```

```{python}
##for jenny
## since you need the date column to be a date, I will convert it for you

final_df['Date'] = pd.to_datetime(final_df['Date'], errors='coerce')

# Check the data type to confirm
print(final_df.dtypes)
print(final_df.head(-5))
```



## Making the scraper dynamic

### 1.

a. 
the pseudo code for writing the function will be like: 
1. going thorugh every row in the df Summer has created. 

2. extracting the dates from date column

3. there will be two types of date

    I. after 2013 >> continue with the rest of function

    II. before 2013 >> show me an error sign that says this is before our desired timeline

4. save all the extracted dates on a csv file called "enfrocment_actions_month_year.csv"

5. do not push it to git

6. print the head


b.

```{python}
desired_dates= []
for index, row in final_df.iterrows():
    date = row['Date']
    
    # Check if date is before or after 2013
    if date.year >= 2013:
        # If date is after 2013, add it to the list
        desired_dates.append(date)
    else:
        # If date is before 2013, print an error message
        print("outside desired period")

# Create a new DataFrame to save the valid dates
desired_dates_df = pd.DataFrame(desired_dates, columns=['Date'])

desired_dates_df.to_csv("enforcement_actions_month_year.csv", index=False)
print(desired_dates_df.head(-5))
```


c.

```{python}

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

def scrape_all_actions(base_url, start_date):
    all_data = []
    page_number = 1
    date_reached = False

    while not date_reached:
        current_url = f"{base_url}?page={page_number}"
        try:
            response = requests.get(current_url)
            if response.status_code != 200:
                print(f"Failed to fetch data from {current_url}, status code: {response.status_code}")
                break
        except requests.exceptions.RequestException as e:
            print(f"Request failed: {e}")
            break

        soup = BeautifulSoup(response.content, 'html.parser')
        actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')

        for action in actions:
            title_tag = action.find('h2', class_='usa-card__heading')
            title = title_tag.get_text(strip=True) if title_tag else 'No Title Provided'
            link = title_tag.find('a')['href'] if title_tag and title_tag.find('a') else 'No Link Provided'
            link = f"https://oig.hhs.gov{link}" if link.startswith('/') else link

            date_div = action.find('div', class_='font-body-sm margin-top-1')
            date_text = date_div.find('span', class_='text-base-dark padding-right-105').get_text(strip=True) if date_div else ''
            try:
                date_obj = datetime.strptime(date_text, '%B %d, %Y')
                if date_obj < start_date:
                    date_reached = True
                    break  # Break the inner loop if we reach before 2021
            except ValueError:
                print(f"Date parsing failed for: {date_text}")
                continue  # Skip this entry if the date is invalid

            category_ul = action.find('ul', class_='display-inline add-list-reset')
            category = category_ul.find('li').get_text(strip=True) if category_ul and category_ul.find('li') else 'No Category Provided'

            all_data.append({'Title': title, 'Date': date_text, 'Category': category, 'Link': link})

        if not date_reached:
            page_number += 1  # Increment page number only if we haven't reached the start date

    return pd.DataFrame(all_data)

# Base URL of the site to scrape
base_url = 'https://oig.hhs.gov/fraud/enforcement/'
start_date = datetime(2021, 1, 1)

# Scrape the data
final_df = scrape_all_actions(base_url, start_date)
print("Total actions since January 2021:", len(final_df))
earliest_action = final_df.sort_values(by='Date').iloc[0]
print("Earliest enforcement action details:", earliest_action)
```

the pseudo code:
1. go thorugh every row in the original df
2. go to column "date" and extract the year part of it if it is 2021 or more
    II. if not 2021 or more, break out of loop (this is a loop inside of the bigger loop)
3. for those whose date meet our requirement, go to the "link" column
4. go over each link on every page and then if we are still within the desired timeframe, go to next online page

## Plot data based on scraped data (using Altair)


### 1.



### 2.




## Create maps of enforcement activity


### 1.


### 2.




## Extra Credit: Calculate the enforcement actions on a per-capita basis

### 1.



### 2.



### 3.













