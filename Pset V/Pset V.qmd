---
title: "Pset V"
author: "Summer Negahdar & Jenny Zhong"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
      text: |
        \usepackage[margin=0.7in]{geometry}
        \usepackage{fvextra}
        \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
execute:
  code-fold: True

---
Partner 1: Summer Negahdar(samarneg)
Partner 2: Jenny Zhong (jzhong1)
This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: **__** **__**
“I have uploaded the names of anyone else other than my partner and I worked with on
the problem set here”
Late coins used this pset: **__** Late coins left after submission: **__**

### A NOTE TO INSTRUCTORS

working on writing the function to scrape and crawl, we found the questions very ocnfusing in terms of task allocation to each partner and therefore instead of writing one short loop for first part and then writing a wholistic loop to scrape all pages we only wrote one gigantic function to scrape everything!
your initials to indicate your agreement: **SN** **JZ**
“I have uploaded the names of anyone else other than my partner and I worked with on the problem set here”

```{python}
from bs4 import BeautifulSoup
import pandas as pd
import requests
import altair
import requests
import datetime
from time import sleep
import os
```

## Develop Initial scraper and crawler

### 1.

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
import time

def fetch_page_data(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    data = []
    actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')
    for action in actions:
        title_tag = action.find('h2', class_='usa-card__heading')
        title = title_tag.get_text(strip=True) if title_tag else 'No Title Provided'
        
        link = title_tag.find('a')['href'] if title_tag and title_tag.find('a') else 'No Link Provided'
        link = f"https://oig.hhs.gov{link}" if link.startswith('/') else link

        # Correctly locate the date using the new structure
        date_div = action.find('div', class_='font-body-sm margin-top-1')
        date = date_div.find('span', class_='text-base-dark padding-right-105').get_text(strip=True) if date_div else 'No Date Provided'
        
        # Correctly locate the category using the new structure
        category_ul = action.find('ul', class_='display-inline add-list-reset')
        category = category_ul.find('li').get_text(strip=True) if category_ul and category_ul.find('li') else 'No Category Provided'

        data.append({'Title': title, 'Date': date, 'Category': category, 'Link': link})
    return data

def scrape_all_pages(base_url, num_pages):
    urls = [f"{base_url}?page={i}" for i in range(1, num_pages + 1)]
    all_data = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(fetch_page_data, urls))
    for result in results:
        all_data.extend(result)
    return pd.DataFrame(all_data)

# Base URL of the site to scrape
base_url = 'https://oig.hhs.gov/fraud/enforcement/'

num_pages = 482  

# Scrape the data
final_df = scrape_all_pages(base_url, num_pages)

print(final_df.head(-5))
print( "number of items found is", len(final_df))
```

### 2.

```{python}
from concurrent.futures import ThreadPoolExecutor, as_completed

def fetch_agency(link):
    try:
        response = requests.get(link)  # Request the page using the link
        soup = BeautifulSoup(response.text, 'html.parser')  # Parse the content of the page

        # Find all <ul> elements with the specified class containing the agency details
        uls = soup.find_all("ul", class_="usa-list usa-list--unstyled margin-y-2")
        agency_name = 'N/A'  # Default if no agency info is found

        # Iterate over each <ul> element
        for ul in uls:
            spans = ul.find_all("span", class_="padding-right-2 text-base")
            if len(spans) > 1:
                agency = spans[1]  # Select the second <span>, which contains "Agency:"
                agency_name = agency.next_sibling.strip() if agency.next_sibling else 'N/A'
                break  # Stop after finding the first matching <ul> and <span> structure

        return agency_name

    except requests.exceptions.RequestException as e:
        print(f"Error fetching {link}: {e}")
        return 'N/A'

def clean_agency_data(agency):
    # Check if the agency data contains a semicolon which might indicate the inclusion of a date
    if ';' in agency:
        parts = agency.split(';')
        if len(parts) > 1:
            return parts[1].strip()
    return agency.strip()

def get_agencies(df):
    agencies = ['N/A'] * len(df)  # Initialize a list of 'N/A' for all agencies
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_index = {executor.submit(fetch_agency, row['Link']): index for index, row in df.iterrows() if row['Link']}
        for future in as_completed(future_to_index):
            index = future_to_index[future]
            raw_agency = future.result()
            cleaned_agency = clean_agency_data(raw_agency)  # Clean the agency data
            agencies[index] = cleaned_agency
    return agencies

def scrape_all_pages(base_url, num_pages):
    urls = [f"{base_url}?page={i}" for i in range(1, num_pages + 1)]
    all_data = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(fetch_agency, urls))
    for result in results:
        all_data.extend(result)
    return pd.DataFrame(all_data)

# Base URL of the site to scrape
base_url = 'https://oig.hhs.gov/fraud/enforcement/'

# Determine the number of pages or set a fixed number if known
num_pages = 482 

# Scrape the data
final_df = scrape_all_pages(base_url, num_pages)

# Add cleaned agency data to DataFrame
final_df['Agency'] = get_agencies(final_df)

# Print the first few rows of the DataFrame to check
print(final_df.head())

# Save the DataFrame to a CSV file
final_df.to_csv("enforcement_actions_all_pages.csv", index=False)
print("Data scraped and saved to enforcement_actions_all_pages.csv")

```

```{python}
final_df['Date'] = pd.to_datetime(final_df['Date'], errors='coerce')

# Check the data type to confirm
print(final_df.dtypes)
print(final_df.head(-5))
```

## Making the scraper dynamic

### 1.

a. 
Step 1: Set up function and input validation 
Function: dynamic_scraper(month, year), month and year are parameters. Check if year is at least 2013; if not, print a warning and exit function. 

Step 2: Initialize URL and variables
Set the base URL for the enforcement actions page. Initialize an empty list to store the data and create a datetime object, base_date_obj, to represent the starting date using month and year. Initialize end_flag as True to contol page looping. 

Step 3: Loop through pages until end condition
Use a while loop that continues as long as end_flag is True. For each page, retrieve the HTML content and parse it using BeautifulSoup to locate the list of enforcement actions. 

Step 4: Extract data from each enforcement action
For each action on the page, extract: title, data (converted to datetime for comparison), category and link. If the date is older than base_date_obj, set end_flag to False to stop the loop. For each link, visit the details page to retrieve the agency, name. 

Step 5: Pause and continue to the next page
Add a 1-second delay between pages to prevent server-side blockng. If a "next page" link exists, update the URL and continue, otherwise exit the loop. 

Step 6: Save data to CSV and print preview 
Convert the collected data to a pandas DataFrame. Save the DataFrame to a CSV file named enforcement_actions_year_month.csv and print the first few rows for verification. 

b. Based on the output, the total number of enforcement actions collected is 20. The date of the earliest enforcement actions is October 30th 2024. 

The details of the earliest enforcement action is "Quincy-Based Physician Group To Pay $650,000 To Resolve Allegations Of False Billing To MassHealth". 

```{python}
desired_dates= []
for index, row in final_df.iterrows():
    date = row['Date']
    
    # Check if date is before or after 2013
    if date.year >= 2013:
        # If date is after 2013, add it to the list
        desired_dates.append(date)
    else:
        # If date is before 2013, print an error message
        print("outside desired period")

# Create a new DataFrame to save the valid dates
desired_dates_df = pd.DataFrame(desired_dates, columns=['Date'])

desired_dates_df.to_csv("enforcement_actions_month_year.csv", index=False)
print(desired_dates_df.head(-5))
def dynamic_scraper(month, year):
    if year < 2013:
        print("Please enter a year greater than or equal to 2013.")
        return
    
    base_url = 'https://oig.hhs.gov'
    url = f'{base_url}/fraud/enforcement/?page=1'
    data = []
    base_date_obj = datetime.datetime(year, month, 1)
    end_flag = True

    while end_flag:
        response = requests.get(url)
        page = BeautifulSoup(response.content, 'html.parser')

        actions = page.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')
        for action in actions:
            date_text = action.find('span', class_="text-base-dark padding-right-105").get_text(strip=True)
            date_object = datetime.datetime.strptime(date_text, "%B %d, %Y")

            if date_object < base_date_obj:
                end_flag = False
                break

            title = action.find('h2', class_='usa-card__heading').get_text(strip=True)
            link = action.find('h2', class_='usa-card__heading').find('a')['href']
            full_link = f"{base_url}{link}"
            category = action.find('ul', class_="display-inline add-list-reset").get_text(strip=True)

            agency = "N/A"
            try:
                detail_response = requests.get(full_link)
                detail_page = BeautifulSoup(detail_response.content, 'html.parser')
                agency_tag = detail_page.find('ul', class_="usa-list usa-list--unstyled margin-y-2")
                if agency_tag:
                    for li in agency_tag.find_all('li'):
                        if "Agency:" in li.get_text():
                            agency = li.get_text().replace("Agency:", "").strip()
                            break
            except requests.exceptions.RequestException as e:
                print(f"Error fetching agency details for {title}: {e}")

            data.append({
                'Title': title,
                'Date': date_text,
                'Category': category,
                'Link': full_link,
                'Agency': agency
            })

        sleep(1)
        try:
            next_page = page.find('li', class_='next-page').find('a')['href']
            url = f"{base_url}{next_page}"
        except AttributeError:
            break

    output_filename = f"enforcement_actions_{year}_{month:02}.csv"
    df = pd.DataFrame(data)
    df.to_csv(output_filename, index=False)
    print(f"Data saved to {output_filename}")
    print(df.head())  

    return df

df = dynamic_scraper(1, 2023)

if not df.empty:
    earliest_action = df.iloc[-1]
    print("\nEarliest enforcement action scraped:")
    print(f"Date: {earliest_action['Date']}")
    print(f"Title: {earliest_action['Title']}")
    print(f"Category: {earliest_action['Category']}")
    print(f"Agency: {earliest_action['Agency']}")
    print(f"Link: {earliest_action['Link']}")

print(f"\nTotal enforcement actions collected: {len(df)}")
```


c.

```{python}

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

def scrape_all_actions(base_url, start_date):
    all_data = []
    page_number = 1
    date_reached = False

    while not date_reached:
        current_url = f"{base_url}?page={page_number}"
        try:
            response = requests.get(current_url)
            if response.status_code != 200:
                print(f"Failed to fetch data from {current_url}, status code: {response.status_code}")
                break
        except requests.exceptions.RequestException as e:
            print(f"Request failed: {e}")
            break

        soup = BeautifulSoup(response.content, 'html.parser')
        actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')

        for action in actions:
            title_tag = action.find('h2', class_='usa-card__heading')
            title = title_tag.get_text(strip=True) if title_tag else 'No Title Provided'
            link = title_tag.find('a')['href'] if title_tag and title_tag.find('a') else 'No Link Provided'
            link = f"https://oig.hhs.gov{link}" if link.startswith('/') else link

            date_div = action.find('div', class_='font-body-sm margin-top-1')
            date_text = date_div.find('span', class_='text-base-dark padding-right-105').get_text(strip=True) if date_div else ''
            try:
                date_obj = datetime.strptime(date_text, '%B %d, %Y')
                if date_obj < start_date:
                    date_reached = True
                    break  # Break the inner loop if we reach before 2021
            except ValueError:
                print(f"Date parsing failed for: {date_text}")
                continue  # Skip this entry if the date is invalid

            category_ul = action.find('ul', class_='display-inline add-list-reset')
            category = category_ul.find('li').get_text(strip=True) if category_ul and category_ul.find('li') else 'No Category Provided'

            all_data.append({'Title': title, 'Date': date_text, 'Category': category, 'Link': link})

        if not date_reached:
            page_number += 1  # Increment page number only if we haven't reached the start date

    return pd.DataFrame(all_data)

# Base URL of the site to scrape
base_url = 'https://oig.hhs.gov/fraud/enforcement/'
start_date = datetime(2021, 1, 1)

# Scrape the data
final_df = scrape_all_actions(base_url, start_date)
print("Total actions since January 2021:", len(final_df))
earliest_action = final_df.sort_values(by='Date').iloc[0]
print("Earliest enforcement action details:", earliest_action)
```

the pseudo code:
1. go thorugh every row in the original df
2. go to column "date" and extract the year part of it if it is 2021 or more
    II. if not 2021 or more, break out of loop (this is a loop inside of the bigger loop)
3. for those whose date meet our requirement, go to the "link" column
4. go over each link on every page and then if we are still within the desired timeframe, go to next online page
c. 

## Plot data based on scraped data (using Altair)


### 1.



### 2.




## Create maps of enforcement activity


### 1.


### 2.




## Extra Credit: Calculate the enforcement actions on a per-capita basis

### 1.



### 2.



### 3.













