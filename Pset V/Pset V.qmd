---
title: "Pset V"
author: "Summer Negahdar & Jenny Zhong"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
      text: |
        \usepackage[margin=0.7in]{geometry}
        \usepackage{fvextra}
        \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
execute:
  code-fold: True

---
Partner 1: Summer Negahdar(samarneg)
Partner 2: Jenny Zhong (jzhong1)
This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: **SN** **JZ**
“I have uploaded the names of anyone else other than my partner and I worked with on the problem set here”

```{python}
from bs4 import BeautifulSoup
import pandas as pd
import requests
import altair
import requests
import datetime
from time import sleep
import os
```

## Develop Initial scraper and crawler

### 1.

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
import time

def fetch_page_data(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    data = []
    actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')
    for action in actions:
        title_tag = action.find('h2', class_='usa-card__heading')
        title = title_tag.get_text(strip=True) if title_tag else 'No Title Provided'
        
        link = title_tag.find('a')['href'] if title_tag and title_tag.find('a') else 'No Link Provided'
        link = f"https://oig.hhs.gov{link}" if link.startswith('/') else link

        # Correctly locate the date using the new structure
        date_div = action.find('div', class_='font-body-sm margin-top-1')
        date = date_div.find('span', class_='text-base-dark padding-right-105').get_text(strip=True) if date_div else 'No Date Provided'
        
        # Correctly locate the category using the new structure
        category_ul = action.find('ul', class_='display-inline add-list-reset')
        category = category_ul.find('li').get_text(strip=True) if category_ul and category_ul.find('li') else 'No Category Provided'

        data.append({'Title': title, 'Date': date, 'Category': category, 'Link': link})
    return data

def scrape_all_pages(base_url, num_pages):
    urls = [f"{base_url}?page={i}" for i in range(1, num_pages + 1)]
    all_data = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(fetch_page_data, urls))
    for result in results:
        all_data.extend(result)
    return pd.DataFrame(all_data)

# Base URL of the site to scrape
base_url = 'https://oig.hhs.gov/fraud/enforcement/'

num_pages = 482  

# Scrape the data
final_df = scrape_all_pages(base_url, num_pages)

print(final_df.head(-5))
print( "number of items found is", len(final_df))
```

### 2.

```{python}
from concurrent.futures import ThreadPoolExecutor, as_completed

def fetch_agency(link):
    try:
        response = requests.get(link)  # Request the page using the link
        soup = BeautifulSoup(response.text, 'html.parser')  # Parse the content of the page

        # Find all <ul> elements with the specified class containing the agency details
        uls = soup.find_all("ul", class_="usa-list usa-list--unstyled margin-y-2")
        agency_name = 'N/A'  # Default if no agency info is found

        # Iterate over each <ul> element
        for ul in uls:
            spans = ul.find_all("span", class_="padding-right-2 text-base")
            if len(spans) > 1:
                agency = spans[1]  # Select the second <span>, which contains "Agency:"
                agency_name = agency.next_sibling.strip() if agency.next_sibling else 'N/A'
                break  # Stop after finding the first matching <ul> and <span> structure

        return agency_name

    except requests.exceptions.RequestException as e:
        print(f"Error fetching {link}: {e}")
        return 'N/A'

def clean_agency_data(agency):
    # Check if the agency data contains a semicolon which might indicate the inclusion of a date
    if ';' in agency:
        parts = agency.split(';')
        if len(parts) > 1:
            return parts[1].strip()
    return agency.strip()

def get_agencies(df):
    agencies = ['N/A'] * len(df)  # Initialize a list of 'N/A' for all agencies
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_index = {executor.submit(fetch_agency, row['Link']): index for index, row in df.iterrows() if row['Link']}
        for future in as_completed(future_to_index):
            index = future_to_index[future]
            raw_agency = future.result()
            cleaned_agency = clean_agency_data(raw_agency)  # Clean the agency data
            agencies[index] = cleaned_agency
    return agencies

def scrape_all_pages(base_url, num_pages):
    urls = [f"{base_url}?page={i}" for i in range(1, num_pages + 1)]
    all_data = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(fetch_agency, urls))
    for result in results:
        all_data.extend(result)
    return pd.DataFrame(all_data)

# Base URL of the site to scrape
base_url = 'https://oig.hhs.gov/fraud/enforcement/'

# Determine the number of pages or set a fixed number if known
num_pages = 482 

# Scrape the data
final_df = scrape_all_pages(base_url, num_pages)

# Add cleaned agency data to DataFrame
final_df['Agency'] = get_agencies(final_df)

# Print the first few rows of the DataFrame to check
print(final_df.head())

# Save the DataFrame to a CSV file
final_df.to_csv("enforcement_actions_all_pages.csv", index=False)
print("Data scraped and saved to enforcement_actions_all_pages.csv")

```

```{python}
final_df['Date'] = pd.to_datetime(final_df['Date'], errors='coerce')

# Check the data type to confirm
print(final_df.dtypes)
print(final_df.head())
```

## Making the scraper dynamic

### 1.

a. 
Step 1: Set up function and input validation 
Function: dynamic_scraper(month, year), month and year are parameters. Check if year is at least 2013; if not, print a warning and exit function. 

Step 2: Initialize URL and variables
Set the base URL for the enforcement actions page. Initialize an empty list to store the data and create a datetime object, base_date_obj, to represent the starting date using month and year. Initialize end_flag as True to contol page looping. 

Step 3: Loop through pages until end condition
Use a while loop that continues as long as end_flag is True. For each page, retrieve the HTML content and parse it using BeautifulSoup to locate the list of enforcement actions. 

Step 4: Extract data from each enforcement action
For each action on the page, extract: title, data (converted to datetime for comparison), category and link. If the date is older than base_date_obj, set end_flag to False to stop the loop. For each link, visit the details page to retrieve the agency, name. 

Step 5: Pause and continue to the next page
Add a 1-second delay between pages to prevent server-side blockng. If a "next page" link exists, update the URL and continue, otherwise exit the loop. 

Step 6: Save data to CSV and print preview 
Convert the collected data to a pandas DataFrame. Save the DataFrame to a CSV file named enforcement_actions_year_month.csv and print the first few rows for verification. 

b. Based on the output, the total number of enforcement actions collected is 20. The date of the earliest enforcement actions is October 30th 2024. 

The details of the earliest enforcement action is "Quincy-Based Physician Group To Pay $650,000 To Resolve Allegations Of False Billing To MassHealth". 

```{python}
def dynamic_scraper(month, year):
    if year < 2013:
        print("Please enter a year greater than or equal to 2013.")
        return
    
    base_url = 'https://oig.hhs.gov'
    url = f'{base_url}/fraud/enforcement/?page=1'
    data = []
    base_date_obj = datetime.datetime(year, month, 1)
    end_flag = True

    while end_flag:
        response = requests.get(url)
        page = BeautifulSoup(response.content, 'html.parser')

        actions = page.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')
        for action in actions:
            date_text = action.find('span', class_="text-base-dark padding-right-105").get_text(strip=True)
            date_object = datetime.datetime.strptime(date_text, "%B %d, %Y")

            if date_object < base_date_obj:
                end_flag = False
                break

            title = action.find('h2', class_='usa-card__heading').get_text(strip=True)
            link = action.find('h2', class_='usa-card__heading').find('a')['href']
            full_link = f"{base_url}{link}"
            category = action.find('ul', class_="display-inline add-list-reset").get_text(strip=True)

            agency = "N/A"
            try:
                detail_response = requests.get(full_link)
                detail_page = BeautifulSoup(detail_response.content, 'html.parser')
                agency_tag = detail_page.find('ul', class_="usa-list usa-list--unstyled margin-y-2")
                if agency_tag:
                    for li in agency_tag.find_all('li'):
                        if "Agency:" in li.get_text():
                            agency = li.get_text().replace("Agency:", "").strip()
                            break
            except requests.exceptions.RequestException as e:
                print(f"Error fetching agency details for {title}: {e}")

            data.append({
                'Title': title,
                'Date': date_text,
                'Category': category,
                'Link': full_link,
                'Agency': agency
            })

        sleep(1)
        try:
            next_page = page.find('li', class_='next-page').find('a')['href']
            url = f"{base_url}{next_page}"
        except AttributeError:
            break

    output_filename = f"enforcement_actions_{year}_{month:02}.csv"
    df = pd.DataFrame(data)
    df.to_csv(output_filename, index=False)
    print(f"Data saved to {output_filename}")
    print(df.head())  

    return df

df = dynamic_scraper(1, 2023)

if not df.empty:
    earliest_action = df.iloc[-1]
    print("\nEarliest enforcement action scraped:")
    print(f"Date: {earliest_action['Date']}")
    print(f"Title: {earliest_action['Title']}")
    print(f"Category: {earliest_action['Category']}")
    print(f"Agency: {earliest_action['Agency']}")
    print(f"Link: {earliest_action['Link']}")

print(f"\nTotal enforcement actions collected: {len(df)}")
```


c. 

## Plot data based on scraped data (using Altair)


### 1.



### 2.




## Create maps of enforcement activity


### 1.


### 2.




## Extra Credit: Calculate the enforcement actions on a per-capita basis

### 1.



### 2.



### 3.













