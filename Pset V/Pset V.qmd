---
title: "Pset V"
author: "Summer Negahdar & Jenny Zhong"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
      text: |
        \usepackage[margin=0.7in]{geometry}
        \usepackage{fvextra}
        \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
execute:
  code-fold: True

---
Partner 1: Summer Negahdar(samarneg)
Partner 2: 
This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: **__** **__**
“I have uploaded the names of anyone else other than my partner and I worked with on
the problem set here”
Late coins used this pset: **__** Late coins left after submission: **__**

```{python}
from bs4 import BeautifulSoup
import pandas as pd
import requests
import altair
import requests
import datetime
from time import sleep
import os
```

## Develop Initial scraper and crawler

### 1.

```{python}
#doing the intial action to get the link parsed
url = 'https://oig.hhs.gov/fraud/enforcement/' # the link we would be scraping
requested = requests.get(url) 
soup = BeautifulSoup(requested.content, 'html.parser')

# Find all actions based on the main <li> tag containing each card
actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')

dataset = [] #creating an ampty list to store craped data

for items in actions:
    title_tag = items.find('h2', class_='usa-card__heading') #tag for the title of enforcement is h2
    if title_tag:
        title = title_tag.get_text(strip=True)  
        link = title_tag.find('a')['href'] if title_tag.find('a') else None #the tag for hyperlinks
        link = f"https://oig.hhs.gov{link}" if link else None  # Complete relative link

    #looking for dates
    date_tag = items.find(lambda tag: tag.name == "span" and "text-base-dark" in tag.get("class", []) and "padding-right-105" in tag.get("class", []))
    date = date_tag.get_text(strip=True) if date_tag else None

    #now we will be looking for category
    category_tag = items.find(lambda tag: tag.name == "ul" and "display-inline" in tag.get("class", []) and "add-list-reset" in tag.get("class", []))
    category = None
    if category_tag:
        li_tag = category_tag.find(lambda tag: tag.name == "li" and "display-inline-block" in tag.get("class", []) and "usa-tag" in tag.get("class", []))
        category = li_tag.get_text(strip=True) if li_tag else None

    # Append data to dataset
    dataset.append({
        'Title': title, 
        'Date': date, 
        'Category': category, 
        'Link': link 
    })

final_df = pd.DataFrame(dataset)
print(final_df.head(5))
```

### 2.

```{python}
# Initialize an empty list to store agency names
agencies = []

# Loop through each link in final_df
for index, row in final_df.iterrows():
    link = row['Link']
    if link:  # Only proceed if the link is valid
        try:
            response = requests.get(link)  # Request the page using the link
            soup = BeautifulSoup(response.text, 'html.parser')  # Parse the content of the page

            # Find all <ul> elements with the specified class containing the agency details
            uls = soup.find_all("ul", class_="usa-list usa-list--unstyled margin-y-2")
            
            # Initialize a placeholder for the agency name
            agency_name = 'N/A'
            
            # Iterate over each <ul> element
            for ul in uls:
                # Find all <span> elements within each <ul> that match the class
                spans = ul.find_all("span", class_="padding-right-2 text-base")
                
                # Ensure there are enough <span> tags to avoid IndexError
                if len(spans) > 1:
                    agency = spans[1]  # Select the second <span>, which contains "Agency:"
                    
                    # Use next_sibling to access the text following the <span>
                    agency_name = agency.next_sibling.strip() if agency.next_sibling else 'N/A'
                    
                    # Stop after finding the first matching <ul> and <span> structure
                    break
            
            # Append the extracted agency name to the agencies list
            agencies.append(agency_name)

        except requests.exceptions.RequestException as e:
            print(f"Error fetching {link}: {e}")
            agencies.append('N/A')

# Add agency names to the DataFrame and print its head
final_df['Agency'] = agencies  # Create a new column in our original df called Agency
print(final_df.head(5))
```

```{python}
final_df['Date'] = pd.to_datetime(final_df['Date'], errors='coerce')

# Check the data type to confirm
print(final_df.dtypes)
print(final_df.head())
```

## Making the scraper dynamic

### 1.

a. 
Step 1: Set up function and input validation 
Function: dynamic_scraper(month, year), month and year are parameters. Check if year is at least 2013; if not, print a warning and exit function. 

Step 2: Initialize URL and variables
Set the base URL for the enforcement actions page. Initialize an empty list to store the data and create a datetime object, base_date_obj, to represent the starting date using month and year. Initialize end_flag as True to contol page looping. 

Step 3: Loop through pages until end condition
Use a while loop that continues as long as end_flag is True. For each page, retrieve the HTML content and parse it using BeautifulSoup to locate the list of enforcement actions. 

Step 4: Extract data from each enforcement action
For each action on the page, extract: title, data (converted to datetime for comparison), category and link. If the date is older than base_date_obj, set end_flag to False to stop the loop. For each link, visit the details page to retrieve the agency, name. 

Step 5: Pause and continue to the next page
Add a 1-second delay between pages to prevent server-side blockng. If a "next page" link exists, update the URL and continue, otherwise exit the loop. 

Step 6: Save data to CSV and print preview 
Convert the collected data to a pandas DataFrame. Save the DataFrame to a CSV file named enforcement_actions_year_month.csv and print the first few rows for verification. 

b. Based on the output, the total number of enforcement actions collected is 20. The date of the earliest enforcement actions is October 30th 2024. 

The details of the earliest enforcement action is "Quincy-Based Physician Group To Pay $650,000 To Resolve Allegations Of False Billing To MassHealth". 

```{python}
def dynamic_scraper(month, year):
    if year < 2013:
        print("Please enter a year greater than or equal to 2013.")
        return
    
    base_url = 'https://oig.hhs.gov'
    url = f'{base_url}/fraud/enforcement/?page=1'
    data = []
    base_date_obj = datetime.datetime(year, month, 1)
    end_flag = True

    while end_flag:
        response = requests.get(url)
        page = BeautifulSoup(response.content, 'html.parser')

        actions = page.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')
        for action in actions:
            date_text = action.find('span', class_="text-base-dark padding-right-105").get_text(strip=True)
            date_object = datetime.datetime.strptime(date_text, "%B %d, %Y")

            if date_object < base_date_obj:
                end_flag = False
                break

            title = action.find('h2', class_='usa-card__heading').get_text(strip=True)
            link = action.find('h2', class_='usa-card__heading').find('a')['href']
            full_link = f"{base_url}{link}"
            category = action.find('ul', class_="display-inline add-list-reset").get_text(strip=True)

            agency = "N/A"
            try:
                detail_response = requests.get(full_link)
                detail_page = BeautifulSoup(detail_response.content, 'html.parser')
                agency_tag = detail_page.find('ul', class_="usa-list usa-list--unstyled margin-y-2")
                if agency_tag:
                    for li in agency_tag.find_all('li'):
                        if "Agency:" in li.get_text():
                            agency = li.get_text().replace("Agency:", "").strip()
                            break
            except requests.exceptions.RequestException as e:
                print(f"Error fetching agency details for {title}: {e}")

            data.append({
                'Title': title,
                'Date': date_text,
                'Category': category,
                'Link': full_link,
                'Agency': agency
            })

        sleep(1)
        try:
            next_page = page.find('li', class_='next-page').find('a')['href']
            url = f"{base_url}{next_page}"
        except AttributeError:
            break

    output_filename = f"enforcement_actions_{year}_{month:02}.csv"
    df = pd.DataFrame(data)
    df.to_csv(output_filename, index=False)
    print(f"Data saved to {output_filename}")
    print(df.head())  

    return df

df = dynamic_scraper(1, 2023)

if not df.empty:
    earliest_action = df.iloc[-1]
    print("\nEarliest enforcement action scraped:")
    print(f"Date: {earliest_action['Date']}")
    print(f"Title: {earliest_action['Title']}")
    print(f"Category: {earliest_action['Category']}")
    print(f"Agency: {earliest_action['Agency']}")
    print(f"Link: {earliest_action['Link']}")

print(f"\nTotal enforcement actions collected: {len(df)}")
```


c. 
## Plot data based on scraped data (using Altair)


### 1.



### 2.




## Create maps of enforcement activity


### 1.


### 2.




## Extra Credit: Calculate the enforcement actions on a per-capita basis

### 1.



### 2.



### 3.













