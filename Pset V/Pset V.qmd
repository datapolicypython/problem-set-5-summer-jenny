---
title: "Pset V"
author: "Summer Negahdar & Jenny Zhong"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
      text: |
        \usepackage[margin=0.7in]{geometry}
        \usepackage{fvextra}
        \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
execute:
  code-fold: True

---
Partner 1: Summer Negahdar(samarneg)
Partner 2: 
This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: **__** **__**
“I have uploaded the names of anyone else other than my partner and I worked with on
the problem set here”
Late coins used this pset: **__** Late coins left after submission: **__**



## Develop Initial scraper and crawler

### 1.

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
import time

def fetch_page_data(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    data = []
    actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')
    for action in actions:
        title_tag = action.find('h2', class_='usa-card__heading')
        title = title_tag.get_text(strip=True) if title_tag else 'No Title Provided'
        
        link = title_tag.find('a')['href'] if title_tag and title_tag.find('a') else 'No Link Provided'
        link = f"https://oig.hhs.gov{link}" if link.startswith('/') else link

        # Correctly locate the date using the new structure
        date_div = action.find('div', class_='font-body-sm margin-top-1')
        date = date_div.find('span', class_='text-base-dark padding-right-105').get_text(strip=True) if date_div else 'No Date Provided'
        
        # Correctly locate the category using the new structure
        category_ul = action.find('ul', class_='display-inline add-list-reset')
        category = category_ul.find('li').get_text(strip=True) if category_ul and category_ul.find('li') else 'No Category Provided'

        data.append({'Title': title, 'Date': date, 'Category': category, 'Link': link})
    return data

def scrape_all_pages(base_url, num_pages):
    urls = [f"{base_url}?page={i}" for i in range(1, num_pages + 1)]
    all_data = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(fetch_page_data, urls))
    for result in results:
        all_data.extend(result)
    return pd.DataFrame(all_data)

# Base URL of the site to scrape
base_url = 'https://oig.hhs.gov/fraud/enforcement/'

num_pages = 482  

# Scrape the data
final_df = scrape_all_pages(base_url, num_pages)

print(final_df.head(-5))
print( "number of items found is", len(final_df))
```

### 2.

```{python}
# Initialize an empty list to store agency names
agencies = []

# Loop through each link in final_df
for index, row in final_df.iterrows():
    link = row['Link']
    if link:  # Only proceed if the link is valid
        try:
            response = requests.get(link)  # Request the page using the link
            soup = BeautifulSoup(response.text, 'html.parser')  # Parse the content of the page

            # Find all <ul> elements with the specified class containing the agency details
            uls = soup.find_all("ul", class_="usa-list usa-list--unstyled margin-y-2")
            
            # Initialize a placeholder for the agency name
            agency_name = 'N/A'
            
            # Iterate over each <ul> element
            for ul in uls:
                # Find all <span> elements within each <ul> that match the class
                spans = ul.find_all("span", class_="padding-right-2 text-base")
                
                # Ensure there are enough <span> tags to avoid IndexError
                if len(spans) > 1:
                    agency = spans[1]  # Select the second <span>, which contains "Agency:"
                    
                    # Use next_sibling to access the text following the <span>
                    agency_name = agency.next_sibling.strip() if agency.next_sibling else 'N/A'
                    
                    # Stop after finding the first matching <ul> and <span> structure
                    break
            
            # Append the extracted agency name to the agencies list
            agencies.append(agency_name)

        except requests.exceptions.RequestException as e:
            print(f"Error fetching {link}: {e}")
            agencies.append('N/A')

# Add agency names to the DataFrame and print its head
final_df['Agency'] = agencies  # Create a new column in our original df called Agency
print(final_df.head(-5))
##there is one row that has different agency info:
# Manually correct the agency information for the second row

```

```{python}
##for jenny
## since you need the date column to be a date, I will convert it for you

final_df['Date'] = pd.to_datetime(final_df['Date'], errors='coerce')

# Check the data type to confirm
print(final_df.dtypes)
print(final_df.head())
```



## Making the scraper dynamic

### 1.

a. 
the pseudo code for writing the function will be like: 
1. going thorugh every row in the df Summer has created. 

2. extracting the dates from date column

3. there will be two types of date

    I. after 2013 >> continue with the rest of function

    II. before 2013 >> show me an error sign that says this is before our desired timeline

4. save all the extracted dates on a csv file called "enfrocment_actions_month_year.csv"

5. do not push it to git

6. print the head


b.

```{python}
desired_dates= []
for index, row in final_df.iterrows():
    date = row['Date']
    
    # Check if date is before or after 2013
    if date.year >= 2013:
        # If date is after 2013, add it to the list
        desired_dates.append(date)
    else:
        # If date is before 2013, print an error message
        print("outside desired period")

# Create a new DataFrame to save the valid dates
desired_dates_df = pd.DataFrame(desired_dates, columns=['Date'])

desired_dates_df.to_csv("enforcement_actions_month_year.csv", index=False)
print("Unique years in the data:", final_df['Date'].dt.year.unique())
```


c.

## Plot data based on scraped data (using Altair)


### 1.



### 2.




## Create maps of enforcement activity


### 1.


### 2.




## Extra Credit: Calculate the enforcement actions on a per-capita basis

### 1.



### 2.



### 3.













