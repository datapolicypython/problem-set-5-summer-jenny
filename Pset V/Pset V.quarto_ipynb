{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Pset V\"\n",
        "author: \"Summer Negahdar & Jenny Zhong\"\n",
        "format: \n",
        "  pdf:\n",
        "    keep-tex: true\n",
        "    include-in-header: \n",
        "      text: |\n",
        "        \\usepackage[margin=0.7in]{geometry}\n",
        "        \\usepackage{fvextra}\n",
        "        \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "execute:\n",
        "  code-fold: True\n",
        "\n",
        "---\n",
        "\n",
        "Partner 1: Summer Negahdar(samarneg)\n",
        "Partner 2: Jenny Zhong (jzhong1)\n",
        "This submission is our work alone and complies with the 30538 integrity policy.” Add your initials to indicate your agreement: **SN** **JZ**\n",
        "\n",
        "we have been facing some bugs when scraping. it either does not show the link or the agency or when we subset it for the years 2021 onwards, it takes aways either of this columns. with that, we have decided to just submit it as it is since through trying to modify it other parts of the code broke (you can check our git history) therefore we have decided not to move forward with the mapping section as it requires data form other parts that are not performable\n"
      ],
      "id": "68bd78a9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "import altair\n",
        "import requests\n",
        "import datetime\n",
        "from time import sleep\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ],
      "id": "41ed4366",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Develop Initial scraper and crawler\n",
        "\n",
        "### 1."
      ],
      "id": "453cb18b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "| eval: false\n",
        "def fetch_page_data(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch data from {url}\")\n",
        "        return []  # Return empty list if failed to fetch data\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    data = []\n",
        "    actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')\n",
        "    for action in actions:\n",
        "        title_tag = action.find('h2', class_='usa-card__heading')\n",
        "        title = title_tag.get_text(strip=True) if title_tag else 'No Title Provided'\n",
        "        \n",
        "        link_tag = title_tag.find('a')\n",
        "        link = f\"https://oig.hhs.gov{link_tag['href']}\" if link_tag else 'No Link Provided'\n",
        "\n",
        "        date_div = action.find('div', class_='font-body-sm margin-top-1')\n",
        "        date_tag = date_div.find('span', class_='text-base-dark padding-right-105')\n",
        "        date = date_tag.get_text(strip=True) if date_tag else 'No Date Provided'\n",
        "        \n",
        "        category_ul = action.find('ul', class_='display-inline add-list-reset')\n",
        "        category_tag = category_ul.find('li')\n",
        "        category = category_tag.get_text(strip=True) if category_tag else 'No Category Provided'\n",
        "\n",
        "        data.append({'Title': title, 'Date': date, 'Category': category, 'Link': link})\n",
        "    return data\n",
        "\n",
        "# Base URL of the site to scrape\n",
        "base_url = 'https://oig.hhs.gov/fraud/enforcement/'\n",
        "\n",
        "# Fetch data from the first page only\n",
        "first_page_data = fetch_page_data(base_url)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(first_page_data)\n",
        "\n",
        "# this scraping is only done on the first page!\n",
        "print(df.head(5))"
      ],
      "id": "f0bbc18b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.\n"
      ],
      "id": "ac071091"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "| eval: false\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def fetch_agency_details(link):\n",
        "    \"\"\" Fetches the agency details for a given URL \"\"\"\n",
        "    try:\n",
        "        response = requests.get(link, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            uls = soup.find_all(\"ul\", class_=\"usa-list usa-list--unstyled margin-y-2\")\n",
        "            for ul in uls:\n",
        "                spans = ul.find_all(\"span\", class_=\"padding-right-2 text-base\")\n",
        "                if len(spans) > 1:\n",
        "                    agency = spans[1]\n",
        "                    return agency.next_sibling.strip() if agency.next_sibling else 'N/A'\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {link}: {e}\")\n",
        "    return 'N/A'\n",
        "\n",
        "def process_agencies(df):\n",
        "    \"\"\" Process each link concurrently and fetch agency details \"\"\"\n",
        "    links = df['Link'].tolist()\n",
        "    agencies = []\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = {executor.submit(fetch_agency_details, link): link for link in links}\n",
        "        for future in as_completed(futures):\n",
        "            agencies.append(future.result())\n",
        "    return agencies\n",
        "\n",
        "# Assuming df is already loaded with the first page data and contains 'Link'\n",
        "if 'Link' in df:\n",
        "    df['Agency'] = process_agencies(df)\n",
        "    print(df.head(5))\n",
        "\n",
        "# Parse dates and check data types\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "print(df.dtypes)\n",
        "print(df.head())"
      ],
      "id": "1e3c4b8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "final_df['Date'] = pd.to_datetime(final_df['Date'])\n",
        "\n",
        "# Check the data type to confirm\n",
        "print(df.dtypes)\n",
        "print(df.head())\n",
        "print(len(df))"
      ],
      "id": "a65543b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making the scraper dynamic\n",
        "\n",
        "### 1.\n",
        "\n",
        "a. \n",
        "Step 1: Set up function and input validation \n",
        "Function: dynamic_scraper(month, year), month and year are parameters. Check if year is at least 2013; if not, print a warning and exit function. \n",
        "\n",
        "Step 2: Initialize URL and variables\n",
        "Set the base URL for the enforcement actions page. Initialize an empty list to store the data and create a datetime object, base_date_obj, to represent the starting date using month and year. Initialize end_flag as True to contol page looping. \n",
        "\n",
        "Step 3: Loop through pages until end condition\n",
        "Use a while loop that continues as long as end_flag is True. For each page, retrieve the HTML content and parse it using BeautifulSoup to locate the list of enforcement actions. \n",
        "\n",
        "Step 4: Extract data from each enforcement action\n",
        "For each action on the page, extract: title, data (converted to datetime for comparison), category and link. If the date is older than base_date_obj, set end_flag to False to stop the loop. For each link, visit the details page to retrieve the agency, name. \n",
        "\n",
        "Step 5: Pause and continue to the next page\n",
        "Add a 1-second delay between pages to prevent server-side blockng. If a \"next page\" link exists, update the URL and continue, otherwise exit the loop. \n",
        "\n",
        "Step 6: Save data to CSV and print preview \n",
        "Convert the collected data to a pandas DataFrame. Save the DataFrame to a CSV file named enforcement_actions_year_month.csv and print the first few rows for verification. \n",
        "\n",
        "b. Based on the output, the total number of enforcement actions collected is 1534. \n",
        "Total enforcement actions collected: 1534\n",
        "Earliest enforcement action scraped:\n",
        "Date: 2023-01-03 00:00:00\n",
        "Title: Podiatrist Pays $90,000 To Settle False Billing Allegations\n",
        "Category: Criminal and Civil Actions\n",
        "Agency: \n",
        "Link: https://oig.hhs.gov/fraud/enforcement/podiatrist-pays-90000-to-settle-false-billing-allegations/\n"
      ],
      "id": "f79534a7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "| eval: false\n",
        "from time import sleep\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "def scrape_all_pages(base_url, base_date_obj):\n",
        "    all_data = []\n",
        "    page_number = 1\n",
        "    stop_scraping = False\n",
        "\n",
        "    while not stop_scraping:\n",
        "        url = f\"{base_url}?page={page_number}\"\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')\n",
        "\n",
        "        if not actions:\n",
        "            break  \n",
        "\n",
        "        for action in actions:\n",
        "            title_tag = action.find('h2', class_='usa-card__heading').find('a')\n",
        "            title = title_tag.get_text(strip=True)\n",
        "            link = f\"https://oig.hhs.gov{title_tag['href']}\"\n",
        "\n",
        "            date_text = action.find('span', class_='text-base-dark padding-right-105').get_text(strip=True)\n",
        "            date_object = datetime.strptime(date_text, \"%B %d, %Y\")\n",
        "\n",
        "            if date_object < base_date_obj:\n",
        "                stop_scraping = True\n",
        "                break\n",
        "\n",
        "            category = action.find('ul', class_='display-inline add-list-reset').get_text(strip=True)\n",
        "\n",
        "            all_data.append({'Title': title, 'Date': date_text, 'Category': category, 'Link': link})\n",
        "\n",
        "        page_number += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "def fetch_agency(link):\n",
        "    try:\n",
        "        response = requests.get(link)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        agency_section = soup.find(\"ul\", class_=\"usa-list usa-list--unstyled margin-y-2\")\n",
        "        if agency_section:\n",
        "            for item in agency_section.find_all('li'):\n",
        "                if \"Agency:\" in item.get_text():\n",
        "                    return item.get_text(strip=True).replace(\"Agency: \", \"\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {link}: {e}\")\n",
        "    return 'N/A'\n",
        "\n",
        "def get_agencies(df):\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        future_to_index = {executor.submit(fetch_agency, row['Link']): index for index, row in df.iterrows()}\n",
        "        agencies = ['N/A'] * len(df)\n",
        "        for future in as_completed(future_to_index):\n",
        "            index = future_to_index[future]\n",
        "            agencies[index] = future.result()\n",
        "    return agencies\n",
        "\n",
        "def scrape_enforcement_actions(month, year):\n",
        "    base_url = 'https://oig.hhs.gov/fraud/enforcement/'\n",
        "    base_date_obj = datetime(year, month, 1)\n",
        "\n",
        "    # Scrape all pages starting from the base date\n",
        "    df = scrape_all_pages(base_url, base_date_obj)\n",
        "    \n",
        "    # Fetch agencies for each enforcement action\n",
        "    df['Agency'] = get_agencies(df)\n",
        "    \n",
        "    # Define the output filename\n",
        "    output_file = f\"enforcement_actions_since_{year}_{month}.csv\"\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "df = scrape_enforcement_actions(1, 2023)\n",
        "if df is not None:\n",
        "    print(df.head())"
      ],
      "id": "277065e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checking the number of rows and the details. "
      ],
      "id": "b9111caa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "| eval: false\n",
        "output_file = \"enforcement_actions_since_2023_1.csv\" \n",
        "df = pd.read_csv(output_file)\n",
        "\n",
        "num_enforcement_actions = len(df)\n",
        "print(f\"Total enforcement actions collected: {num_enforcement_actions}\")\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "earliest_action = df.sort_values(by='Date').iloc[0]\n",
        "\n",
        "print(\"\\nEarliest enforcement action scraped:\")\n",
        "print(f\"Date: {earliest_action['Date']}\")\n",
        "print(f\"Title: {earliest_action['Title']}\")\n",
        "print(f\"Category: {earliest_action['Category']}\")\n",
        "print(f\"Agency: {earliest_action['Agency']}\")\n",
        "print(f\"Link: {earliest_action['Link']}\")"
      ],
      "id": "03ce13c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. "
      ],
      "id": "e3f048b9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "| eval: false\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from time import sleep\n",
        "\n",
        "def scrape_enforcement_actions(base_url, start_date):\n",
        "    all_data = []\n",
        "    page_number = 1\n",
        "    stop_scraping = False\n",
        "\n",
        "    while not stop_scraping:\n",
        "        url = f\"{base_url}?page={page_number}\"\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        \n",
        "        # Find all enforcement actions on the page\n",
        "        actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')\n",
        "        if not actions:\n",
        "            break\n",
        "\n",
        "        for action in actions:\n",
        "            # Extract title, link, and date\n",
        "            title_tag = action.find('h2', class_='usa-card__heading').find('a')\n",
        "            title = title_tag.get_text(strip=True)\n",
        "            link = f\"https://oig.hhs.gov{title_tag['href']}\"\n",
        "\n",
        "            date_text = action.find('span', class_='text-base-dark padding-right-105').get_text(strip=True)\n",
        "            date_object = datetime.strptime(date_text, \"%B %d, %Y\")\n",
        "\n",
        "            # Stop scraping if the date is before the start date\n",
        "            if date_object < start_date:\n",
        "                stop_scraping = True\n",
        "                break\n",
        "\n",
        "            # Extract category and agency\n",
        "            category = action.find('ul', class_='display-inline add-list-reset').get_text(strip=True)\n",
        "            agency = fetch_agency(link)\n",
        "\n",
        "            # Append data to list\n",
        "            all_data.append({'Title': title, 'Date': date_text, 'Category': category, 'Link': link, 'Agency': agency})\n",
        "\n",
        "        page_number += 1\n",
        "        sleep(1)  # Add a delay to avoid overloading the server\n",
        "\n",
        "    # Convert to DataFrame and save to CSV\n",
        "    df = pd.DataFrame(all_data)\n",
        "    output_file = \"enforcement_actions_since_jan2021.csv\"\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def fetch_agency(link):\n",
        "    try:\n",
        "        response = requests.get(link)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        agency_section = soup.find(\"ul\", class_=\"usa-list usa-list--unstyled margin-y-2\")\n",
        "        if agency_section:\n",
        "            for item in agency_section.find_all('li'):\n",
        "                if \"Agency:\" in item.get_text():\n",
        "                    return item.get_text(strip=True).replace(\"Agency: \", \"\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching agency from {link}: {e}\")\n",
        "    return 'N/A'\n",
        "\n",
        "# Scrape all enforcement actions from January 2021 onwards\n",
        "base_url = 'https://oig.hhs.gov/fraud/enforcement/'\n",
        "start_date1 = datetime(2021, 1, 1)\n",
        "df1 = scrape_enforcement_actions(base_url, start_date1)\n",
        "\n",
        "# Display summary information\n",
        "num_enforcement_actions = len(df)\n",
        "print(f\"Total enforcement actions collected: {num_enforcement_actions}\")\n",
        "\n",
        "# Convert 'Date' column to datetime format and find the earliest action\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "earliest_action = df.sort_values(by='Date').iloc[0]\n",
        "\n",
        "print(\"\\nEarliest enforcement action scraped:\")\n",
        "print(f\"Date: {earliest_action['Date']}\")\n",
        "print(f\"Title: {earliest_action['Title']}\")\n",
        "print(f\"Category: {earliest_action['Category']}\")\n",
        "print(f\"Agency: {earliest_action['Agency']}\")\n",
        "print(f\"Link: {earliest_action['Link']}\")"
      ],
      "id": "215638d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot data based on scraped data (using Altair)\n",
        "\n",
        "### 1."
      ],
      "id": "93b8a94a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_2021 = pd.read_csv(\"enforcement_actions_since202101.csv\")\n",
        "\n",
        "df_2021['date'] = pd.to_datetime(df_2021['date'], errors='coerce')\n",
        "\n",
        "df_2021 = df_2021.dropna(subset=['date'])\n",
        "\n",
        "df_2021['YearMonth'] = df_2021['date'].dt.to_period('M')\n",
        "monthly_counts = df_2021.groupby('YearMonth').size().reset_index(name='ActionCount')\n",
        "monthly_counts['YearMonth'] = monthly_counts['YearMonth'].dt.to_timestamp()\n",
        "\n",
        "chart_step31 = alt.Chart(monthly_counts).mark_line(point=True).encode(\n",
        "    x=alt.X('YearMonth:T', title='Date (Month-Year)'),\n",
        "    y=alt.Y('ActionCount:Q', title='Number of Enforcement Actions')\n",
        ").properties(\n",
        "    title=\"Number of Enforcement Actions Over Time (Monthly Aggregation)\",\n",
        "    width=600,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "chart_step31.display()"
      ],
      "id": "2c2d1e29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Number of enforcement actions over time](q31.png)\n",
        "\n",
        "### 2."
      ],
      "id": "0b2d171e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_2021 = pd.read_csv(\"enforcement_actions_since202101.csv\")\n",
        "\n",
        "df_2021['date'] = pd.to_datetime(df_2021['date'], errors='coerce')\n",
        "df_2021 = df_2021.dropna(subset=['date']) \n",
        "\n",
        "def categorize_title(title):\n",
        "    title = title.lower()\n",
        "    if any(word in title for word in ['health', 'healthcare', 'medicare', 'medical']):\n",
        "        return 'Health Care Fraud'\n",
        "    elif any(word in title for word in ['bank', 'financial', 'money']):\n",
        "        return 'Financial Fraud'\n",
        "    elif 'drug' in title or 'pharmacy' in title:\n",
        "        return 'Drug Enforcement'\n",
        "    elif any(word in title for word in ['bribe', 'corruption', 'corrupt']):\n",
        "        return 'Bribery/Corruption'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "df_2021['grouped_category'] = df_2021['title'].apply(categorize_title)\n",
        "\n",
        "df_2021['YearMonth'] = df_2021['date'].dt.to_period('M')\n",
        "monthly_counts = df_2021.groupby(['YearMonth', 'grouped_category']).size().reset_index(name='ActionCount')\n",
        "monthly_counts['YearMonth'] = monthly_counts['YearMonth'].dt.to_timestamp()\n",
        "\n",
        "chart32 = alt.Chart(monthly_counts).mark_line(point=True).encode(\n",
        "    x=alt.X('YearMonth:T', title='Date (Month-Year)'),\n",
        "    y=alt.Y('ActionCount:Q', title='Number of Enforcement Actions'),\n",
        "    color='grouped_category:N'  # Different colors for each subcategory\n",
        ").properties(\n",
        "    title=\"Number of Enforcement Actions by Subcategory Over Time (Monthly Aggregation)\",\n",
        "    width=600,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "chart32.display()"
      ],
      "id": "9cce67f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Number of enforcement actions by subcategory over time](q32.png)\n",
        "\n",
        "## Create maps of enforcement activity\n",
        "\n",
        "### 1.\n",
        "\n",
        "### 2."
      ],
      "id": "f58bd57c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "print(df_2021.head())"
      ],
      "id": "85ac6708",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "usattorney = gpd.read_file(\"42data.shp\")\n",
        "\n",
        "I ran into the same error as before although it was fixed and I could not debug. My apologies: CRSError: Invalid projection: EPSG:4326: (Internal Proj Error: proj_create: no database context specified)"
      ],
      "id": "f728bc2f",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/jennyzhong/Documents/GitHub/problem-set-5-summer-jenny/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}