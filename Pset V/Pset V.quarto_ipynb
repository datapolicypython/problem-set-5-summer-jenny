{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Pset V\"\n",
        "author: \"Summer Negahdar & Jenny Zhong\"\n",
        "format: \n",
        "  pdf:\n",
        "    keep-tex: true\n",
        "    include-in-header: \n",
        "      text: |\n",
        "        \\usepackage[margin=0.7in]{geometry}\n",
        "        \\usepackage{fvextra}\n",
        "        \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "execute:\n",
        "  code-fold: True\n",
        "\n",
        "---\n",
        "\n",
        "Partner 1: Summer Negahdar(samarneg)\n",
        "Partner 2: \n",
        "This submission is our work alone and complies with the 30538 integrity policy.” Add\n",
        "your initials to indicate your agreement: **__** **__**\n",
        "“I have uploaded the names of anyone else other than my partner and I worked with on\n",
        "the problem set here”\n",
        "Late coins used this pset: **__** Late coins left after submission: **__**\n",
        "\n",
        "\n",
        "\n",
        "## Develop Initial scraper and crawler\n",
        "\n",
        "### 1.\n"
      ],
      "id": "6d3847e3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_page(url):\n",
        "    response = requests.get(url)\n",
        "    return response.text\n",
        "\n",
        "def parse_page(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    data = []\n",
        "    actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')\n",
        "    for action in actions:\n",
        "        # Extract and append data like before\n",
        "        # ...\n",
        "    return data\n",
        "\n",
        "def scrape_all_pages(base_url, num_pages):\n",
        "    all_data = []\n",
        "    urls = [f\"{base_url}?page={i}\" for i in range(1, num_pages + 1)]\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        html_contents = list(executor.map(fetch_page, urls))\n",
        "        for html in html_contents:\n",
        "            all_data.extend(parse_page(html))\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "# Use the function to scrape data\n",
        "final_df = scrape_all_pages('https://oig.hhs.gov/fraud/enforcement/', 482)"
      ],
      "id": "d45b586a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.\n"
      ],
      "id": "ec33e213"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize an empty list to store agency names\n",
        "agencies = []\n",
        "\n",
        "# Loop through each link in final_df\n",
        "for index, row in final_df.iterrows():\n",
        "    link = row['Link']\n",
        "    if link:  # Only proceed if the link is valid\n",
        "        try:\n",
        "            response = requests.get(link)  # Request the page using the link\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')  # Parse the content of the page\n",
        "\n",
        "            # Find all <ul> elements with the specified class containing the agency details\n",
        "            uls = soup.find_all(\"ul\", class_=\"usa-list usa-list--unstyled margin-y-2\")\n",
        "            \n",
        "            # Initialize a placeholder for the agency name\n",
        "            agency_name = 'N/A'\n",
        "            \n",
        "            # Iterate over each <ul> element\n",
        "            for ul in uls:\n",
        "                # Find all <span> elements within each <ul> that match the class\n",
        "                spans = ul.find_all(\"span\", class_=\"padding-right-2 text-base\")\n",
        "                \n",
        "                # Ensure there are enough <span> tags to avoid IndexError\n",
        "                if len(spans) > 1:\n",
        "                    agency = spans[1]  # Select the second <span>, which contains \"Agency:\"\n",
        "                    \n",
        "                    # Use next_sibling to access the text following the <span>\n",
        "                    agency_name = agency.next_sibling.strip() if agency.next_sibling else 'N/A'\n",
        "                    \n",
        "                    # Stop after finding the first matching <ul> and <span> structure\n",
        "                    break\n",
        "            \n",
        "            # Append the extracted agency name to the agencies list\n",
        "            agencies.append(agency_name)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching {link}: {e}\")\n",
        "            agencies.append('N/A')\n",
        "\n",
        "# Add agency names to the DataFrame and print its head\n",
        "final_df['Agency'] = agencies  # Create a new column in our original df called Agency\n",
        "print(final_df.head(5))\n",
        "##there is one row that has different agency info:\n",
        "# Manually correct the agency information for the second row"
      ],
      "id": "dac97013",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "##for jenny\n",
        "## since you need the date column to be a date, I will convert it for you\n",
        "\n",
        "final_df['Date'] = pd.to_datetime(final_df['Date'], errors='coerce')\n",
        "\n",
        "# Check the data type to confirm\n",
        "print(final_df.dtypes)\n",
        "print(final_df.head())"
      ],
      "id": "89ab1483",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making the scraper dynamic\n",
        "\n",
        "### 1.\n",
        "\n",
        "a. \n",
        "the pseudo code for writing the function will be like: \n",
        "1. going thorugh every row in the df Summer has created. \n",
        "\n",
        "2. extracting the dates from date column\n",
        "\n",
        "3. there will be two types of date\n",
        "\n",
        "    I. after 2013 >> continue with the rest of function\n",
        "\n",
        "    II. before 2013 >> show me an error sign that says this is before our desired timeline\n",
        "\n",
        "4. save all the extracted dates on a csv file called \"enfrocment_actions_month_year.csv\"\n",
        "\n",
        "5. do not push it to git\n",
        "\n",
        "6. print the head\n",
        "\n",
        "\n",
        "b.\n"
      ],
      "id": "b9af857a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "desired_dates= []\n",
        "for index, row in final_df.iterrows():\n",
        "    date = row['Date']\n",
        "    \n",
        "    # Check if date is before or after 2013\n",
        "    if date.year >= 2013:\n",
        "        # If date is after 2013, add it to the list\n",
        "        desired_dates.append(date)\n",
        "    else:\n",
        "        # If date is before 2013, print an error message\n",
        "        print(\"outside desired period\")\n",
        "\n",
        "# Create a new DataFrame to save the valid dates\n",
        "desired_dates_df = pd.DataFrame(desired_dates, columns=['Date'])\n",
        "\n",
        "desired_dates_df.to_csv(\"enforcement_actions_month_year.csv\", index=False)\n",
        "print(\"Unique years in the data:\", final_df['Date'].dt.year.unique())"
      ],
      "id": "c3508abb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c.\n",
        "\n",
        "## Plot data based on scraped data (using Altair)\n",
        "\n",
        "\n",
        "### 1.\n",
        "\n",
        "\n",
        "\n",
        "### 2.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Create maps of enforcement activity\n",
        "\n",
        "\n",
        "### 1.\n",
        "\n",
        "\n",
        "### 2.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Extra Credit: Calculate the enforcement actions on a per-capita basis\n",
        "\n",
        "### 1.\n",
        "\n",
        "\n",
        "\n",
        "### 2.\n",
        "\n",
        "\n",
        "\n",
        "### 3.\n"
      ],
      "id": "c3371c9f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/samarnegahdar/Desktop/untitled folder/problem-set-5-summer-jenny/Pset V/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}