{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Pset V\"\n",
        "author: \"Summer Negahdar & Jenny Zhong\"\n",
        "format: \n",
        "  pdf:\n",
        "    keep-tex: true\n",
        "    include-in-header: \n",
        "      text: |\n",
        "        \\usepackage[margin=0.7in]{geometry}\n",
        "        \\usepackage{fvextra}\n",
        "        \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "execute:\n",
        "  code-fold: True\n",
        "\n",
        "---\n",
        "\n",
        "Partner 1: Summer Negahdar(samarneg)\n",
        "Partner 2: \n",
        "This submission is our work alone and complies with the 30538 integrity policy.” Add\n",
        "your initials to indicate your agreement: **__** **__**\n",
        "“I have uploaded the names of anyone else other than my partner and I worked with on\n",
        "the problem set here”\n",
        "Late coins used this pset: **__** Late coins left after submission: **__**\n"
      ],
      "id": "3ff542b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests"
      ],
      "id": "d27acfec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Develop Initial scraper and crawler\n",
        "\n",
        "### 1.\n"
      ],
      "id": "61054b13"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#doing the intial action to get the link parsed\n",
        "url = 'https://oig.hhs.gov/fraud/enforcement/' # the link we would be scraping\n",
        "requested = requests.get(url) \n",
        "soup = BeautifulSoup(requested.content, 'html.parser')\n",
        "\n",
        "# Find all actions based on the main <li> tag containing each card\n",
        "actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')\n",
        "\n",
        "dataset = [] #creating an ampty list to store craped data\n",
        "\n",
        "for items in actions:\n",
        "    title_tag = items.find('h2', class_='usa-card__heading') #tag for the title of enforcement is h2\n",
        "    if title_tag:\n",
        "        title = title_tag.get_text(strip=True)  \n",
        "        link = title_tag.find('a')['href'] if title_tag.find('a') else None #the tag for hyperlinks\n",
        "        link = f\"https://oig.hhs.gov{link}\" if link else None  # Complete relative link\n",
        "\n",
        "    #looking for dates\n",
        "    date_tag = items.find(lambda tag: tag.name == \"span\" and \"text-base-dark\" in tag.get(\"class\", []) and \"padding-right-105\" in tag.get(\"class\", []))\n",
        "    date = date_tag.get_text(strip=True) if date_tag else None\n",
        "\n",
        "    #now we will be looking for category\n",
        "    category_tag = items.find(lambda tag: tag.name == \"ul\" and \"display-inline\" in tag.get(\"class\", []) and \"add-list-reset\" in tag.get(\"class\", []))\n",
        "    category = None\n",
        "    if category_tag:\n",
        "        li_tag = category_tag.find(lambda tag: tag.name == \"li\" and \"display-inline-block\" in tag.get(\"class\", []) and \"usa-tag\" in tag.get(\"class\", []))\n",
        "        category = li_tag.get_text(strip=True) if li_tag else None\n",
        "\n",
        "    # Append data to dataset\n",
        "    dataset.append({\n",
        "        'Title': title, \n",
        "        'Date': date, \n",
        "        'Category': category, \n",
        "        'Link': link \n",
        "    })\n",
        "\n",
        "final_df = pd.DataFrame(dataset)\n",
        "print(final_df.head(5))"
      ],
      "id": "84fc1402",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.\n"
      ],
      "id": "e2529a2b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize an empty list to store agency names\n",
        "agencies = []\n",
        "\n",
        "# Loop through each link in final_df\n",
        "for index, row in final_df.iterrows():\n",
        "    link = row['Link']\n",
        "    if link:  # Only proceed if the link is valid\n",
        "        try:\n",
        "            response = requests.get(link)  # Request the page using the link\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')  # Parse the content of the page\n",
        "\n",
        "            # Find all <ul> elements with the specified class containing the agency details\n",
        "            uls = soup.find_all(\"ul\", class_=\"usa-list usa-list--unstyled margin-y-2\")\n",
        "            \n",
        "            # Initialize a placeholder for the agency name\n",
        "            agency_name = 'N/A'\n",
        "            \n",
        "            # Iterate over each <ul> element\n",
        "            for ul in uls:\n",
        "                # Find all <span> elements within each <ul> that match the class\n",
        "                spans = ul.find_all(\"span\", class_=\"padding-right-2 text-base\")\n",
        "                \n",
        "                # Ensure there are enough <span> tags to avoid IndexError\n",
        "                if len(spans) > 1:\n",
        "                    agency = spans[1]  # Select the second <span>, which contains \"Agency:\"\n",
        "                    \n",
        "                    # Use next_sibling to access the text following the <span>\n",
        "                    agency_name = agency.next_sibling.strip() if agency.next_sibling else 'N/A'\n",
        "                    \n",
        "                    # Stop after finding the first matching <ul> and <span> structure\n",
        "                    break\n",
        "            \n",
        "            # Append the extracted agency name to the agencies list\n",
        "            agencies.append(agency_name)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching {link}: {e}\")\n",
        "            agencies.append('N/A')\n",
        "\n",
        "# Add agency names to the DataFrame and print its head\n",
        "final_df['Agency'] = agencies  # Create a new column in our original df called Agency\n",
        "print(final_df.head(5))"
      ],
      "id": "0f78a39f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "##for jenny\n",
        "## since you need the date column to be a date, I will convert it for you\n",
        "\n",
        "final_df['date_column'] = pd.to_datetime(df['date_column'], format='%B %d %Y')\n",
        "\n",
        "# Check the data type to confirm\n",
        "print(df.dtypes)\n",
        "print(df.head())"
      ],
      "id": "5973b749",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making the scraper dynamic\n",
        "\n",
        "### 1.\n",
        "the pseudo code is like this: \n",
        "1. take all the ndates we already have in final_df\n",
        "2. there will be two outcomes: before or after 2013\n",
        "    I. if before 2013, then show an error message that this is not the period we want\n",
        "    II. if after 2013, then continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "a.\n",
        "\n",
        "\n",
        "b.\n",
        "\n",
        "\n",
        "c.\n",
        "\n",
        "## Plot data based on scraped data (using Altair)\n",
        "\n",
        "\n",
        "### 1.\n",
        "\n",
        "\n",
        "\n",
        "### 2.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Create maps of enforcement activity\n",
        "\n",
        "\n",
        "### 1.\n",
        "\n",
        "\n",
        "### 2.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Extra Credit: Calculate the enforcement actions on a per-capita basis\n",
        "\n",
        "### 1.\n",
        "\n",
        "\n",
        "\n",
        "### 2.\n",
        "\n",
        "\n",
        "\n",
        "### 3.\n"
      ],
      "id": "c916770a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/samarnegahdar/Desktop/untitled folder/problem-set-5-summer-jenny/Pset V/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}