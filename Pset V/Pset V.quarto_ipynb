{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Pset V\"\n",
        "author: \"Summer Negahdar & Jenny Zhong\"\n",
        "format: \n",
        "  pdf:\n",
        "    keep-tex: true\n",
        "    include-in-header: \n",
        "      text: |\n",
        "        \\usepackage[margin=0.7in]{geometry}\n",
        "        \\usepackage{fvextra}\n",
        "        \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "execute:\n",
        "  code-fold: True\n",
        "\n",
        "---\n",
        "\n",
        "Partner 1: Summer Negahdar(samarneg)\n",
        "Partner 2: Jenny Zhong (jzhong1)\n",
        "This submission is our work alone and complies with the 30538 integrity policy.” Add\n",
        "your initials to indicate your agreement: **SN** **JZ**\n"
      ],
      "id": "77b4dd5f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "import altair\n",
        "import requests\n",
        "import datetime\n",
        "from time import sleep\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ],
      "id": "49778e9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Develop Initial scraper and crawler\n",
        "\n",
        "### 1."
      ],
      "id": "209c7055"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def fetch_page_data(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch data from {url}\")\n",
        "        return []  # Return empty list if failed to fetch data\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    data = []\n",
        "    actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')\n",
        "    for action in actions:\n",
        "        title_tag = action.find('h2', class_='usa-card__heading')\n",
        "        title = title_tag.get_text(strip=True) if title_tag else 'No Title Provided'\n",
        "        \n",
        "        link_tag = title_tag.find('a')\n",
        "        link = f\"https://oig.hhs.gov{link_tag['href']}\" if link_tag else 'No Link Provided'\n",
        "\n",
        "        date_div = action.find('div', class_='font-body-sm margin-top-1')\n",
        "        date_tag = date_div.find('span', class_='text-base-dark padding-right-105')\n",
        "        date = date_tag.get_text(strip=True) if date_tag else 'No Date Provided'\n",
        "        \n",
        "        category_ul = action.find('ul', class_='display-inline add-list-reset')\n",
        "        category_tag = category_ul.find('li')\n",
        "        category = category_tag.get_text(strip=True) if category_tag else 'No Category Provided'\n",
        "\n",
        "        data.append({'Title': title, 'Date': date, 'Category': category, 'Link': link})\n",
        "    return data\n",
        "\n",
        "# Base URL of the site to scrape\n",
        "base_url = 'https://oig.hhs.gov/fraud/enforcement/'\n",
        "\n",
        "# Fetch data from the first page only\n",
        "first_page_data = fetch_page_data(base_url)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(first_page_data)\n",
        "\n",
        "# this scraping is only done on the first page!\n",
        "print(df.head(5))"
      ],
      "id": "53704ac0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.\n"
      ],
      "id": "9229a1e1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def fetch_agency_details(link):\n",
        "    \"\"\" Fetches the agency details for a given URL \"\"\"\n",
        "    try:\n",
        "        response = requests.get(link, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            uls = soup.find_all(\"ul\", class_=\"usa-list usa-list--unstyled margin-y-2\")\n",
        "            for ul in uls:\n",
        "                spans = ul.find_all(\"span\", class_=\"padding-right-2 text-base\")\n",
        "                if len(spans) > 1:\n",
        "                    agency = spans[1]\n",
        "                    return agency.next_sibling.strip() if agency.next_sibling else 'N/A'\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {link}: {e}\")\n",
        "    return 'N/A'\n",
        "\n",
        "def process_agencies(df):\n",
        "    \"\"\" Process each link concurrently and fetch agency details \"\"\"\n",
        "    links = df['Link'].tolist()\n",
        "    agencies = []\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = {executor.submit(fetch_agency_details, link): link for link in links}\n",
        "        for future in as_completed(futures):\n",
        "            agencies.append(future.result())\n",
        "    return agencies\n",
        "\n",
        "# Assuming df is already loaded with the first page data and contains 'Link'\n",
        "if 'Link' in df:\n",
        "    df['Agency'] = process_agencies(df)\n",
        "    print(df.head(5))\n",
        "\n",
        "# Parse dates and check data types\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "print(df.dtypes)\n",
        "print(df.head())"
      ],
      "id": "8933f280",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Check the data type to confirm\n",
        "print(df.dtypes)\n",
        "print(df.head())\n",
        "print(len(df))"
      ],
      "id": "37545659",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making the scraper dynamic\n",
        "\n",
        "### 1.\n",
        "\n",
        "a. \n",
        "Step 1: Set up function and input validation \n",
        "Function: dynamic_scraper(month, year), month and year are parameters. Check if year is at least 2013; if not, print a warning and exit function. \n",
        "\n",
        "Step 2: Initialize URL and variables\n",
        "Set the base URL for the enforcement actions page. Initialize an empty list to store the data and create a datetime object, base_date_obj, to represent the starting date using month and year. Initialize end_flag as True to contol page looping. \n",
        "\n",
        "Step 3: Loop through pages until end condition\n",
        "Use a while loop that continues as long as end_flag is True. For each page, retrieve the HTML content and parse it using BeautifulSoup to locate the list of enforcement actions. \n",
        "\n",
        "Step 4: Extract data from each enforcement action\n",
        "For each action on the page, extract: title, data (converted to datetime for comparison), category and link. If the date is older than base_date_obj, set end_flag to False to stop the loop. For each link, visit the details page to retrieve the agency, name. \n",
        "\n",
        "Step 5: Pause and continue to the next page\n",
        "Add a 1-second delay between pages to prevent server-side blockng. If a \"next page\" link exists, update the URL and continue, otherwise exit the loop. \n",
        "\n",
        "Step 6: Save data to CSV and print preview \n",
        "Convert the collected data to a pandas DataFrame. Save the DataFrame to a CSV file named enforcement_actions_year_month.csv and print the first few rows for verification. \n",
        "\n",
        "b. Based on the output, the total number of enforcement actions collected is 1534. \n",
        "Total enforcement actions collected: 1534\n",
        "Earliest enforcement action scraped:\n",
        "Date: 2023-01-03 00:00:00\n",
        "Title: Podiatrist Pays $90,000 To Settle False Billing Allegations\n",
        "Category: Criminal and Civil Actions\n",
        "Agency: January 3, 2023\n",
        "Link: https://oig.hhs.gov/fraud/enforcement/podiatrist-pays-90000-to-settle-false-billing-allegations/\n"
      ],
      "id": "f2ccf4be"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from time import sleep\n",
        "from datetime import datetime\n",
        "\n",
        "def scrape_all_pages(base_url, month, year, output_file):\n",
        "    page_number = 1\n",
        "    base_date_obj = datetime(year, month, 1) \n",
        "    stop_scraping = False\n",
        "\n",
        "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"Title\", \"Date\", \"Category\", \"Link\"])\n",
        "        writer.writeheader()  # Write the header row once\n",
        "\n",
        "        while not stop_scraping:\n",
        "            url = f\"{base_url}?page={page_number}\"\n",
        "            response = requests.get(url)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')\n",
        "            \n",
        "            if not actions:\n",
        "                print(f\"No more actions found on page {page_number}.\")\n",
        "                break\n",
        "\n",
        "            for action in actions:\n",
        "                title = action.find('h2', class_='usa-card__heading').get_text(strip=True)\n",
        "                link = \"https://oig.hhs.gov\" + action.find('h2').find('a')['href']\n",
        "                date_text = action.find('span', class_='text-base-dark padding-right-105').get_text(strip=True)\n",
        "                date_object = datetime.strptime(date_text, \"%B %d, %Y\")\n",
        "                category = action.find('ul', class_=\"display-inline add-list-reset\").get_text(strip=True)\n",
        "                \n",
        "                if date_object < base_date_obj:\n",
        "                    stop_scraping = True\n",
        "                    break\n",
        "                \n",
        "                writer.writerow({'Title': title, 'Date': date_text, 'Category': category, 'Link': link})\n",
        "            \n",
        "            page_number += 1\n",
        "            sleep(1)  \n",
        "        \n",
        "def fetch_agency(link):\n",
        "    try:\n",
        "        response = requests.get(link)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        agency_tag = soup.find(\"ul\", class_=\"usa-list usa-list--unstyled margin-y-2\")\n",
        "        if agency_tag:\n",
        "            agency = agency_tag.find(\"span\", class_=\"padding-right-2 text-base\")\n",
        "            return agency.next_sibling.strip() if agency and agency.next_sibling else 'N/A'\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {link}: {e}\")\n",
        "    return 'N/A'\n",
        "\n",
        "def add_agencies_to_csv(output_file):\n",
        "    df = pd.read_csv(output_file)\n",
        "    df['Agency'] = get_agencies(df)\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "def get_agencies(df):\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        future_to_index = {executor.submit(fetch_agency, row['Link']): index for index, row in df.iterrows()}\n",
        "        agencies = ['N/A'] * len(df)\n",
        "        for future in as_completed(future_to_index):\n",
        "            index = future_to_index[future]\n",
        "            agencies[index] = future.result()\n",
        "    return agencies\n",
        "\n",
        "base_url = 'https://oig.hhs.gov/fraud/enforcement/'\n",
        "output_file = \"enforcement_actions_since_jan2023.csv\"\n",
        "\n",
        "scrape_all_pages(base_url, 1, 2023, output_file)\n",
        "\n",
        "add_agencies_to_csv(output_file)\n",
        "\n",
        "print(\"Data scraped and saved to enforcement_actions_since_jan2023.csv\")"
      ],
      "id": "05d4e474",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checking the number of rows and the details. "
      ],
      "id": "cdbc4235"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "output_file = \"enforcement_actions_since_jan2023.csv\"\n",
        "df = pd.read_csv(output_file)\n",
        "\n",
        "num_enforcement_actions = len(df)\n",
        "print(f\"Total enforcement actions collected: {num_enforcement_actions}\")\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "earliest_action = df.sort_values(by='Date').iloc[0]\n",
        "\n",
        "print(\"\\nEarliest enforcement action scraped:\")\n",
        "print(f\"Date: {earliest_action['Date']}\")\n",
        "print(f\"Title: {earliest_action['Title']}\")\n",
        "print(f\"Category: {earliest_action['Category']}\")\n",
        "print(f\"Agency: {earliest_action['Agency']}\")\n",
        "print(f\"Link: {earliest_action['Link']}\")"
      ],
      "id": "b7f059a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. "
      ],
      "id": "326edd48"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#using my partner's code to collect all data from 2021\n",
        "\n",
        "base_url = 'https://oig.hhs.gov/fraud/enforcement/'\n",
        "output_file_2021 = \"enforcement_actions_since_jan2021.csv\"\n",
        "\n",
        "scrape_all_pages(base_url, 1, 2021, output_file_2021)\n",
        "\n",
        "add_agencies_to_csv(output_file_2021)\n",
        "df_2021 = pd.read_csv(output_file_2021)\n",
        "\n",
        "num_enforcement_actions_2021 = len(df_2021)\n",
        "print(f\"Total enforcement actions collected since 2021: {num_enforcement_actions_2021}\")\n",
        "\n",
        "df_2021['Date'] = pd.to_datetime(df_2021['Date'], errors='coerce')\n",
        "\n",
        "earliest_action_2021 = df_2021.sort_values(by='Date').iloc[0]\n",
        "\n",
        "print(\"\\nEarliest enforcement action scraped:\")\n",
        "print(f\"Date: {earliest_action_2021['Date']}\")\n",
        "print(f\"Title: {earliest_action_2021['Title']}\")\n",
        "print(f\"Category: {earliest_action_2021['Category']}\")\n",
        "print(f\"Agency: {earliest_action_2021['Agency']}\")\n",
        "print(f\"Link: {earliest_action_2021['Link']}\")"
      ],
      "id": "37d0fd6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot data based on scraped data (using Altair)\n",
        "\n",
        "### 1."
      ],
      "id": "2c754f04"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import altair as alt\n",
        "\n",
        "df_2021 = pd.read_csv(\"enforcement_actions_since_jan2021.csv\")\n",
        "\n",
        "df_2021['Date'] = pd.to_datetime(df_2021['Date'], errors='coerce')\n",
        "\n",
        "print(df_2021.head)\n",
        "\n",
        "df_2021 = df_2021.dropna(subset=['Date'])\n",
        "\n",
        "df_2021['YearMonth'] = df_2021['Date'].dt.to_period('M')\n",
        "monthly_counts = df_2021.groupby('YearMonth').size().reset_index(name='ActionCount')\n",
        "monthly_counts['YearMonth'] = monthly_counts['YearMonth'].dt.to_timestamp()\n",
        "\n",
        "chart_step31 = alt.Chart(monthly_counts).mark_line(point=True).encode(\n",
        "    x=alt.X('YearMonth:T', title='Date (Month-Year)'),\n",
        "    y=alt.Y('ActionCount:Q', title='Number of Enforcement Actions')\n",
        ").properties(\n",
        "    title=\"Number of Enforcement Actions Over Time (Monthly Aggregation)\",\n",
        "    width=600,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "chart_step31.display()"
      ],
      "id": "3d4be5b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.\n"
      ],
      "id": "a437835c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "##the first plot for two distinct categories: \n",
        "\n",
        "df_2021['Date'] = pd.to_datetime(df_2021['Date'])\n",
        "\n",
        "# Mock classification between 'Criminal and Civil Actions' vs. 'State Enforcement Agencies'\n",
        "# Here I'll assume 'Category' tells us this information directly\n",
        "\n",
        "# Group data by Category and count by month\n",
        "grouped_data = df_2021.groupby(['Category']).resample('M', on='Date').size().reset_index(name='Counts')\n",
        "\n",
        "# Plotting the data using Altair\n",
        "chart = alt.Chart(grouped_data).mark_line(point=True).encode(\n",
        "    x='Date:T',\n",
        "    y='Counts:Q',\n",
        "    color='Category:N',\n",
        "    tooltip=['Date:T', 'Counts:Q', 'Category:N']\n",
        ").properties(\n",
        "    title='Number of Enforcement Actions by Category Over Time'\n",
        ").interactive()\n",
        "\n",
        "chart"
      ],
      "id": "2ddf9cdc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assuming 'Category' field categorizes actions into \n",
        "\n",
        "\n",
        "## Create maps of enforcement activity\n",
        "\n",
        "\n",
        "### 1.\n",
        "\n",
        "\n",
        "### 2.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Extra Credit: Calculate the enforcement actions on a per-capita basis\n",
        "\n",
        "### 1.\n",
        "\n",
        "\n",
        "\n",
        "### 2.\n",
        "\n",
        "\n",
        "\n",
        "### 3."
      ],
      "id": "3c333333"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/samarnegahdar/Desktop/untitled folder/problem-set-5-summer-jenny/Pset V/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}